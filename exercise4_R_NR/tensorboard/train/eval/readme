  
  How to read these hyperparameters:
  
  
  
  
    num_actions = 2 # number of actions, predefined by gym env
    state_dim = 4 # same as above
    method = "DQL" # double Q learning, if CQL then standard Q learning
    game = "cartpole" # name of the game
    epsilon = 0.3 # initial epsilon
    epsilon_decay = 0.99 # decay factor
    explore_type = "epsilon_greedy"# explore_type epsilon greedy contains anneal or standard. If decay factor = 1, then it's standard epsilon, if <1, then it is annealing
    epsilon_min = 0.04 # the minimum epsilon after decay (make sure we always explore a bit)
    tau = 2 # the hyperparameters of Boltzmann exploration, if tau -> +inf then it's equally select all actions
    
    # hidden: number of hidden units of Q learning, only valid in cartpole (in carracing we always use IngmarNet)
    # lr: learning rate
    Q = NeuralNetwork(state_dim=state_dim, num_actions=num_actions, hidden=200, lr=3e-4)
    Q_target = TargetNetwork(state_dim=state_dim, num_actions=num_actions, hidden=200, lr=3e-4)
    
    # discount factor: discount_factor we use in Q learning for future actions, in our case it is always 0.98
    agent = DQNAgent(Q, Q_target, num_actions,
                     method=method,
                     discount_factor=0.98,
                     batch_size=64,
                     epsilon=epsilon,
                     epsilon_decay=epsilon_decay,
                     explore_type=explore_type,
                     epsilon_min=epsilon_min,
                     game=game, tau=tau)
                     
    # the last argument, the integer of train_online function is the number of training episodes, but it never reached since the others break the training process, so please use tensorboard to check the number of episodes
    
    # the action probability of carracing is weighted by [2, 5, 5, 10, 1], (LEFT = 1
RIGHT = 2
STRAIGHT = 0
ACCELERATE = 3
BRAKE = 4)

# the tau of behaviour network and target network is 0.01 (as we discussed)
# batch size is always 64, 
    train_online(env, agent, 1600)
    
    
    
    
 Remard:
 For carracing using classical q learning and epsilon anneal, it has only been trained for 550 episodes, but still achieve a very good result in testing. I'm now training it again, and once it finished, I will send you the new result. Sorry for the inconvenince
