  
  ### How to read these hyperparameters:
  
  
  
  ```python
    num_actions = 2 # number of actions, predefined by gym env
    state_dim = 4 # same as above
    method = "DQL" # double Q learning, if CQL then standard Q learning
    game = "cartpole" # name of the game
    epsilon = 0.3 # initial epsilon
    epsilon_decay = 0.99 # decay factor
    explore_type = "epsilon_greedy"# explore_type epsilon greedy contains anneal or standard. If decay factor = 1, then it's standard epsilon, if <1, then it is annealing
    epsilon_min = 0.04 # the minimum epsilon after decay (make sure we always explore a bit)
    tau = 2 # the hyperparameters of Boltzmann exploration, if tau -> +inf then it's equally select all actions
    
    # hidden: number of hidden units of Q learning, only valid in cartpole (in carracing we always use IngmarNet)
    # lr: learning rate
    Q = NeuralNetwork(state_dim=state_dim, num_actions=num_actions, hidden=200, lr=3e-4)
    Q_target = TargetNetwork(state_dim=state_dim, num_actions=num_actions, hidden=200, lr=3e-4)
    
    # discount factor: discount_factor we use in Q learning for future actions, in our case it is always 0.98
    agent = DQNAgent(Q, Q_target, num_actions,
                     method=method,
                     discount_factor=0.98,
                     batch_size=64,
                     epsilon=epsilon,
                     epsilon_decay=epsilon_decay,
                     explore_type=explore_type,
                     epsilon_min=epsilon_min,
                     game=game, tau=tau)
                     
    # the last argument, the integer of train_online function is the number of training episodes, but it never reached since the others break the training process, so please use tensorboard to check the number of episodes
    
    # the action probability of carracing is weighted by [2, 5, 5, 10, 1], 
    (LEFT = 1
    RIGHT = 2
    STRAIGHT = 0
    ACCELERATE = 3
    BRAKE = 4)

    # the tau of behaviour network and target network is 0.01 (as we discussed)
    # batch size is always 64, 
    # the replay_buffer size is: 1e5
    train_online(env, agent, 1600)
    # the max_timesteps are reduced to 300 for the first 30% training episodes
```python
    
    
    