{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhzhang/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:73: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "# %load train_agent.py\n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import Model\n",
    "from utils import *\n",
    "from tensorboard_evaluation import Evaluation\n",
    "import tensorflow as tf\n",
    "\n",
    "def read_data(datasets_dir=\"./data\", frac = 0.1):\n",
    "    \"\"\"\n",
    "    This method reads the states and actions recorded in drive_manually.py \n",
    "    and splits it into training/ validation set.\n",
    "    \"\"\"\n",
    "    print(\"... read data\")\n",
    "    data_file = os.path.join(datasets_dir, 'data.pkl.gzip')\n",
    "  \n",
    "    f = gzip.open(data_file,'rb')\n",
    "    data = pickle.load(f)\n",
    "\n",
    "    # get images as features and actions as targets\n",
    "    X = np.array(data[\"state\"]).astype('float32')\n",
    "    y = np.array(data[\"action\"]).astype('float32')\n",
    "\n",
    "    # split data into training and validation set\n",
    "    n_samples = len(data[\"state\"])\n",
    "    X_train, y_train = X[:int((1-frac) * n_samples)], y[:int((1-frac) * n_samples)]\n",
    "    X_valid, y_valid = X[int((1-frac) * n_samples):], y[int((1-frac) * n_samples):]\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "def data_augmentation(X_train, y_train_id):\n",
    "    # left:\n",
    "    left_indices = (y_train_id == LEFT)\n",
    "    # right:\n",
    "    right_indices = (y_train_id == RIGHT)\n",
    "    X_new_left_data = np.flip(X_train[left_indices], axis=2)\n",
    "    X_new_right_data = np.flip(X_train[right_indices], axis=2)\n",
    "    y_new_left_data = np.zeros((X_new_left_data.shape[0])) + RIGHT\n",
    "    y_new_right_data = np.zeros((X_new_right_data.shape[0])) + LEFT\n",
    "    X_train_n = np.concatenate((X_train, X_new_left_data, X_new_right_data), axis=0)\n",
    "    y_train_id_n = np.concatenate((y_train_id, y_new_left_data, y_new_right_data), axis=0)\n",
    "    \n",
    "    return X_train_n, y_train_id_n\n",
    "\n",
    "def id_to_action(labels_id):\n",
    "    classes = 3\n",
    "    labels_action = np.zeros((labels_id.shape[0], classes))\n",
    "    labels_action[labels_id==LEFT] = [-1.0, 0.0, 0.0]\n",
    "    labels_action[labels_id==RIGHT] = [1.0, 0.0, 0.0]\n",
    "    labels_action[labels_id==STRAIGHT] = [0.0, 0.0, 0.0]\n",
    "    labels_action[labels_id==ACCELERATE] = [0.0, 1.0, 0.0]\n",
    "    labels_action[labels_id==BRAKE] = [0.0, 0.0, 0.8]\n",
    "    return labels_action\n",
    "\n",
    "def uniform_sampling(X_train, y_train_id_n, num_samples):\n",
    "    n = X_train.shape[0]\n",
    "    weights = np.zeros(n)\n",
    "    left_indices = y_train_id_n == LEFT\n",
    "    weights[y_train_id_n == LEFT] = n / np.sum(left_indices)\n",
    "    right_indices = y_train_id_n == RIGHT\n",
    "    weights[y_train_id_n == RIGHT] = n / np.sum(right_indices)\n",
    "    straight_indices = y_train_id_n == STRAIGHT\n",
    "    weights[y_train_id_n == STRAIGHT] = n / np.sum(straight_indices)\n",
    "    acce_indices = y_train_id_n == ACCELERATE\n",
    "    weights[y_train_id_n == ACCELERATE] = n / np.sum(acce_indices)\n",
    "    brake_indices = y_train_id_n == BRAKE\n",
    "    weights[y_train_id_n == BRAKE] = n / np.sum(brake_indices)\n",
    "    weights = weights / np.sum(weights)\n",
    "    samples_indices = np.random.choice(np.arange(n), num_samples, p = weights)\n",
    "    \n",
    "    return X_train[samples_indices], y_train_id_n[samples_indices]\n",
    "\n",
    "\n",
    "def preprocessing(X_train, y_train, X_valid, y_valid, history_length=1):\n",
    "\n",
    "    # TODO: preprocess your data here.\n",
    "    # 1. convert the images in X_train/X_valid to gray scale. If you use rgb2gray() from utils.py, the output shape (96, 96, 1)\n",
    "    # 2. you can either train your model with continous actions (as you get them from read_data) using regression\n",
    "    #    or you discretize the action space using action_to_id() from utils.py. If you discretize them, you'll maybe find one_hot() \n",
    "    #    useful and you may want to return X_train_unhot ... as well.\n",
    "    X_train = rgb2gray(X_train)\n",
    "    X_valid = rgb2gray(X_valid)\n",
    "    # history:\n",
    "    #X_train_history = np.zeros((X_train.shape[0]-history_length+1, history_length, X_train.shape[1], X_train.shape[2]))\n",
    "    #for i in range(X_train_history.shape[0]):\n",
    "    #    X_train_history[i] = X_train[i:i+history_length,:,:]\n",
    "    #X_train_history = X_train_history.transpose(0,2,3,1)\n",
    "    \n",
    "    #X_valid_history = np.zeros((X_valid.shape[0]-history_length+1, history_length, X_valid.shape[1], X_valid.shape[2]))\n",
    "    #for i in range(X_valid_history.shape[0]):\n",
    "    #    X_valid_history[i] = X_valid[i:i+history_length,:,:]\n",
    "    #X_valid_history = X_valid_history.transpose(0,2,3,1)\n",
    "    \n",
    "    # discretize actions\n",
    "    y_train_id = np.zeros(y_train.shape[0])\n",
    "    y_valid_id = np.zeros(y_valid.shape[0])\n",
    "    \n",
    "    # data augmentation\n",
    "    for i in range(y_train.shape[0]):\n",
    "        y_train_id[i] = action_to_id(y_train[i])\n",
    "\n",
    "    X_train_n, y_train_id_n = data_augmentation(X_train, y_train_id)\n",
    "    X_train_sampled, y_train_id_sampled = uniform_sampling(X_train_n, y_train_id_n, num_samples=12000)\n",
    "\n",
    "    y_train_action = id_to_action(y_train_id_sampled)\n",
    "    # History:\n",
    "    # At first you should only use the current image as input to your network to learn the next action. Then the input states\n",
    "    # have shape (96, 96,1). Later, add a history of the last N images to your state so that a state has shape (96, 96, N).\n",
    "    \n",
    "    return X_train_sampled, y_train_action, X_valid, y_valid\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_valid, y_valid, epochs, batch_size, lr, history_length=1, model_dir=\"./models\", tensorboard_dir=\"./tensorboard\"):\n",
    "    \n",
    "    # create result and model folders\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)  \n",
    " \n",
    "    print(\"... train model\")\n",
    "\n",
    "\n",
    "    # TODO: specify your neural network in model.py \n",
    "    agent = Model(learning_rate=lr, history_length=history_length)\n",
    "    init = tf.global_variables_initializer()\n",
    "    agent.sess.run(init)\n",
    "    tensorboard_eval = Evaluation(tensorboard_dir)\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # calculate the accuracy\n",
    "    # y_pred = tf.argmax(agent.output, 1)\n",
    "    # tf.add_to_collection('pred_network', y_pred)\n",
    "    # correct_pred = tf.equal(agent.y_pred, tf.argmax(agent.y_label, 1))\n",
    "    # calculate train and valid accuracy\n",
    "    # accuracy = tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
    "    # init all variables\n",
    "    \n",
    "    # TODO: implement the training\n",
    "    # \n",
    "    # 1. write a method sample_minibatch and perform an update step\n",
    "    # 2. compute training/ validation accuracy and loss for the batch and visualize them with tensorboard. You can watch the progress of\n",
    "    #    your training in your web browser\n",
    "    \n",
    "    total_batch_num = X_train.shape[0] // batch_size;\n",
    "    total_batch_num_valid = X_valid.shape[0] // batch_size;\n",
    "    # training loop\n",
    "    train_cost = np.zeros((epochs))\n",
    "    # train_accuracy = np.zeros((epochs))\n",
    "    valid_cost = np.zeros((epochs))\n",
    "    for epoch in range(epochs):\n",
    "        # shuffle the data set\n",
    "        # index = np.random.permutation(X_train.shape[0])\n",
    "        # X_train, y_train = X_train[index], y_train[index]\n",
    "        for b in range(total_batch_num):\n",
    "            # select the batch data\n",
    "            X_batch = X_train[b*batch_size:(b+1)*batch_size]\n",
    "            y_batch = y_train[b*batch_size:(b+1)*batch_size]\n",
    "            # compute the cost\n",
    "            _ , temp_cost = agent.sess.run([agent.optimizer, agent.cost], feed_dict={agent.x_input:X_batch, agent.y_label:y_batch})\n",
    "\n",
    "        \n",
    "        # training cost\n",
    "        for b in range(total_batch_num):\n",
    "            X_batch = X_train[b*batch_size:(b+1)*batch_size]\n",
    "            y_batch = y_train[b*batch_size:(b+1)*batch_size]\n",
    "            train_cost[epoch] += agent.sess.run(agent.cost, feed_dict={agent.x_input: X_batch, agent.y_label: y_batch})\n",
    "\n",
    "        # validation cost\n",
    "        for b in range(total_batch_num_valid):\n",
    "            X_valid_batch = X_valid[b*batch_size:(b+1)*batch_size]\n",
    "            y_valid_batch = y_valid[b*batch_size:(b+1)*batch_size]\n",
    "            valid_cost[epoch] += agent.sess.run(agent.cost, feed_dict={agent.x_input:X_valid_batch, agent.y_label:y_valid_batch})\n",
    "        train_cost[epoch] = train_cost[epoch] / total_batch_num\n",
    "        valid_cost[epoch] = valid_cost[epoch] / total_batch_num_valid\n",
    "        print(\"[%d/%d]: train_cost: %.4f, valid_cost: %.4f\" %(epoch+1, epochs, train_cost[epoch], valid_cost[epoch]))\n",
    "        eval_dict = {\"train\":train_cost[epoch], \"valid\":valid_cost[epoch]}\n",
    "        tensorboard_eval.write_episode_data(epoch, eval_dict)\n",
    "      \n",
    "    # TODO: save your agent\n",
    "    agent.save(os.path.join(model_dir, \"agent.ckpt\"))\n",
    "    print(model_dir)\n",
    "    print(\"Model saved in file: %s\" % model_dir)\n",
    "    agent.sess.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # read data    \n",
    "    X_train, y_train, X_valid, y_valid = read_data(\"./data\")\n",
    "    history_length = 5\n",
    "    # preprocess data\n",
    "    X_train, y_train, X_valid, y_valid = preprocessing(X_train, y_train, X_valid, y_valid, history_length)\n",
    "    # train model (you can change the parameters!)\n",
    "    # train_model(X_train, y_train, X_valid, y_valid, history_length=history_length, epochs=20, batch_size=64, lr=0.0004)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 96, 96, 5)\n",
      "(64, 3)\n"
     ]
    }
   ],
   "source": [
    "def sample_minibatch(X_train, y_train, b=0, batch_size=64, history_length=1):\n",
    "    X_batch = np.zeros((batch_size, X_train.shape[1], X_train.shape[2], history_length))\n",
    "    for i in range(history_length):\n",
    "        X_batch[:,:,:,i] = X_train[i+b*batch_size:i+(b+1)*batch_size]\n",
    "    y_batch = y_train[history_length-1+b*batch_size:history_length-1+(b+1)*batch_size]\n",
    "    index = np.random.permutation(batch_size)\n",
    "    X_batch, y_batch = X_batch[index], y_batch[index]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "X_batch, y_batch = sample_minibatch(X_valid, y_valid, b=0, batch_size=64, history_length=5)\n",
    "print(X_batch.shape)\n",
    "print(y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
